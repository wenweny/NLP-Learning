{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【NLP序列标注】Keras-BiLSTM-NER | Keras实现中文命名实体识别"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "数据格式要求形如：\n",
    "\n",
    "公司 O\n",
    "战略 O\n",
    "定位 O\n",
    "饲料 B-industry\n",
    "龙头 O\n",
    "企业 O\n",
    "。 O\n",
    "\n",
    "上下游 O\n",
    "延伸 O\n",
    "产业链 O\n",
    "也 O\n",
    "是 O\n",
    "以 O\n",
    "服务 B-industry\n",
    "饲料 I-industry\n",
    "业务 O\n",
    "为主 O\n",
    "。\n",
    "\n",
    "每句话中间以换行分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import platform\n",
    "import numpy\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping,CSVLogger\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM,Bidirectional\n",
    "from keras_contrib.layers import CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取及预处理\n",
    "\n",
    "- 数据读取，分割每句话在一个子列表中\n",
    "- 生成训练数据的对应词表，频次目前取>2的\n",
    "- 对文本x和标签y进行padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_split_text = \" \" # 文本与标签的分隔符空格或者\\t或者其他\n",
    "\n",
    "def read_data(fh):\n",
    "    #  in windows the new line is '\\r\\n\\r\\n' the space is '\\r\\n' . so if you use windows system,\n",
    "    #  you have to use recorsponding instructions\n",
    "\n",
    "    if platform.system() == 'Windows':\n",
    "        split_text = '\\r\\n'\n",
    "    else:\n",
    "        split_text = '\\n'\n",
    "        \n",
    "    # 以字符串读取，全文都在一个字符串上“'从 O\\n效益 O\\n上 O\\n来 O\\n看 O\\n...”\n",
    "    string = fh.read().decode('utf-8')\n",
    "    rows_data = [row.replace(\"#\",\"\") for row in string.strip().split(split_text)]\n",
    "    \n",
    "    # 分割句子\n",
    "    sentence_data = []\n",
    "    sentence_tmp= []\n",
    "    for row in rows_data:\n",
    "        if row.strip():\n",
    "            sentence_tmp.append(row)\n",
    "        else:\n",
    "            sentence_data.append(sentence_tmp)\n",
    "            sentence_tmp = []\n",
    "            \n",
    "    fh.close()\n",
    "    return sentence_data\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    train = read_data(open('../../data/industry/train.txt', 'rb'))\n",
    "    test = read_data(open('../../data/industry/dev.txt', 'rb'))\n",
    "    \n",
    "    word_counts = Counter(word.split(\" \")[0].lower() for sentence in train for word in sentence)\n",
    "    \n",
    "    vocab = [w for w, f in iter(word_counts.items()) if f >= 2]\n",
    "    tags = ['O', 'B-industry', 'I-industry']\n",
    "\n",
    "    # save initial config data\n",
    "    with open('config.pkl', 'wb') as outp:\n",
    "        pickle.dump((vocab, tags), outp)\n",
    "\n",
    "    train = _process_data(train, vocab, tags)\n",
    "    test = _process_data(test, vocab, tags)\n",
    "    return train, test, (vocab, tags)\n",
    "\n",
    "def _process_data(data, vocab, tags, maxlen=100, onehot=False):\n",
    "\n",
    "    word2idx = dict((w, i) for i, w in enumerate(vocab))\n",
    "    idx2word = dict((i, w) for i, w in enumerate(vocab))\n",
    "    \n",
    "    # set to <unk> (index end) if not in vocab\n",
    "    UNK_index = len(vocab)\n",
    "    x = [[word2idx.get(row.split(tag_split_text)[0],UNK_index) for row in s]for s in data]\n",
    "    y = [[tags.index(row.split(tag_split_text)[1]) for row in s] for s in data]\n",
    "\n",
    "    x = pad_sequences(x, maxlen,padding=\"post\",truncating=\"post\")  # Right padding\n",
    "    y = pad_sequences(y, maxlen,padding=\"post\",truncating=\"post\",value=-1)\n",
    "\n",
    "    if onehot:\n",
    "        y = numpy.eye(len(tags), dtype='float32')[y]\n",
    "    else:\n",
    "        y = numpy.expand_dims(y, 2)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构建及训练\n",
    "- 模型是最简单的Embedding+BiLSTM+CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 200)         5490000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 200)         240800    \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, None, 3)           618       \n",
      "=================================================================\n",
      "Total params: 5,731,418\n",
      "Trainable params: 5,731,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras_contrib-2.0.8-py3.6.egg/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "/opt/conda/lib/python3.6/site-packages/keras_contrib-2.0.8-py3.6.egg/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48274 samples, validate on 5373 samples\n",
      "Epoch 1/10\n",
      " 5312/48274 [==>...........................] - ETA: 24:33 - loss: nan - crf_viterbi_accuracy: 0.9228"
     ]
    }
   ],
   "source": [
    "def create_model(input_dim,embed_dim,birnn_units,crf_classify_num):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim, embed_dim, mask_zero=True))  # Random embedding\n",
    "    # 双向RNN包装器\n",
    "    model.add(Bidirectional(LSTM(birnn_units // 2, return_sequences=True)))\n",
    "    crf = CRF(crf_classify_num, sparse_target=True)\n",
    "    model.add(crf)\n",
    "    model.summary()\n",
    "    model.compile('adam', loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "    return model\n",
    "\n",
    "def run_train():\n",
    "    EPOCHS = 10\n",
    "    (train_x, train_y), (test_x, test_y), (vocab, tags) = load_data()\n",
    "    model = create_model(input_dim = len(vocab),embed_dim=200,birnn_units = 200,crf_classify_num=len(tags))\n",
    "    model.fit(train_x, train_y,batch_size=16,epochs=EPOCHS,validation_data=[test_x, test_y])\n",
    "    model.save('crf_test.h5')\n",
    "    \n",
    "run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
