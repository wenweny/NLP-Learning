{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW\n",
    "\n",
    "context --> target\n",
    "\n",
    "和skip_gram 基本相同，输入和输出不同。所以如何创建单词嵌套和从句子中生成嵌套数据与之不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python.framework import ops\n",
    "import collections\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "embedding_size = 200\n",
    "batch_size = 50\n",
    "windows_size = 2\n",
    "num_sampled = int(batch_size/2) #控制多少个批量转换为随机噪声\n",
    "epochs = 1000\n",
    "\n",
    "valid_words = ['cliche','love','hate']\n",
    "\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = 'dataset\\\\rt-polaritydata'\n",
    "\n",
    "def load_data(data_file_path):\n",
    "    pos_file = os.path.join(data_file_path,'rt-polarity.pos')# 正例\n",
    "    neg_file = os.path.join(data_file_path,'rt-polarity.neg')# 负例\n",
    "\n",
    "    #read data\n",
    "    pos_data = []\n",
    "    with open(pos_file,'r',encoding='gb18030',errors='ignore') as temp_pos_file:\n",
    "        for row in temp_pos_file:\n",
    "            pos_data.append(row)\n",
    "\n",
    "    neg_data = []\n",
    "    with open(neg_file,'r',encoding='gb18030',errors='ignore') as temp_neg_file:\n",
    "        for row in temp_neg_file:\n",
    "            neg_data.append(row)\n",
    "\n",
    "    texts = pos_data+neg_data\n",
    "    target = [1]*len(pos_data)+[0]*len(neg_data)\n",
    "    \n",
    "    return texts,target\n",
    "\n",
    "texts,target = load_data(data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.数据预处理\n",
    "#### 2.1 转小写、去除标点数字和空白、去除“停词”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(texts):\n",
    "    stops = stopwords.words('english')#需要提前下载nltk_data，放在指定位置\n",
    "\n",
    "    # Lower case\n",
    "    texts = [x.lower() for x in texts]\n",
    "\n",
    "    # Remove punctuation\n",
    "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "\n",
    "    # Remove numbers\n",
    "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "\n",
    "    # Remove stopwords\n",
    "    texts = [' '.join([word for word in x.split() if word not in (stops)]) for x in texts]\n",
    "\n",
    "    # Trim extra whitespace\n",
    "    texts = [' '.join(x.split()) for x in texts]\n",
    "    \n",
    "    return(texts)\n",
    "\n",
    "texts = normalize_text(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 筛选影评长度大于3的数据，为更好确保影评的有效性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > 2]\n",
    "texts = [x for x in texts if len(x.split()) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.构建词汇表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 统计词频，取频次前voacabulary_size的词语和频次；\n",
    "2. 对每个词语赋予一个数值，这里采用赋予一个新数值 这个词典前面的长度，就可以实现递增唯一赋值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(sentences,vocabulary_size):\n",
    "\n",
    "    # Turn sentences (list of strings) into lists of words\n",
    "    split_sentences = [s.split() for s in sentences]\n",
    "    words = [x for sublist in split_sentences for x in sublist]\n",
    "\n",
    "    # Initialize list of [word, word_count] for each word, starting with unknown\n",
    "    count = [('RARE', -1)]\n",
    "\n",
    "    # Now add most frequent words, limited to the N-most frequent (N=vocabulary size) \n",
    "    # most_common 取出现频次最多的N个\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    \n",
    "    # Now create the dictionary\n",
    "    word_dictionary = {}\n",
    "    # For each word, that we want in the dictionary, add it, then make it\n",
    "    # the value of the prior dictionary length\n",
    "    for i,(word, word_count) in enumerate(count):\n",
    "        word_dictionary[word] = len(word_dictionary)\n",
    "    \n",
    "    return word_dictionary\n",
    "\n",
    "word_dictionary = build_dictionary(texts,vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.对数据集单词进行数值映射\n",
    "当单词的频次在前N个即在word_dict中，即映射相应数值，否则映射rare的0;\n",
    "- input : sentences 里面的元素是英文单词本身\n",
    "- output : text_data 里面的元素是数值，是根据词汇表 word_dictionary 映射英文单词对应的数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_number(sentences):\n",
    "    text_data = []\n",
    "    for sentence in sentences:\n",
    "        sentence_data = []\n",
    "        for word in sentence.split():\n",
    "            if word in word_dictionary:\n",
    "                sentence_data.append(word_dictionary[word])\n",
    "            else:\n",
    "                sentence_data.append(0)\n",
    "        text_data.append(sentence_data)\n",
    "    return text_data\n",
    "\n",
    "text_data = text_to_number(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证集的单词也进行映射，此时验证集单词应选择在前N频次的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1434, 28, 980]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在前N个词汇中选择\n",
    "valid_example = [word_dictionary[x] for x in valid_words]\n",
    "valid_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[184, 2556, 2336, 0],\n",
       "  [2556, 2611, 0, 8989],\n",
       "  [2611, 2336, 8989, 96],\n",
       "  [2336, 0, 96, 546]],\n",
       " [2611, 2336, 0, 8989])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_sentence = np.random.choice(text_data)\n",
    "# Generate consecutive windows to look at\n",
    "window_sequences = [rand_sentence[max((ix - windows_size), 0):(ix + windows_size + 1)] for ix, x in\n",
    "                    enumerate(rand_sentence)]\n",
    "# 选择中心词\n",
    "# Denote which element of each window is the center word of interest\n",
    "label_indices = [ix if ix < windows_size else windows_size for ix, x in enumerate(window_sequences)]\n",
    "\n",
    "# 这里和skip-gram不一样，sk是(中心词，周围词)，而cbow是（[周围词..]，中心词）\n",
    "# Pull out center word of interest for each window and create a tuple for each window\n",
    "batch_and_labels = [(x[:y] + x[(y + 1):], x[y]) for x, y in zip(window_sequences, label_indices)]\n",
    "# Make it in to a big list of tuples (target word, surrounding word)\n",
    "\n",
    "# 此时的x输入是Windows_size大小的周围词，y是中心词,，所以要选择周围词大小等于两倍窗口大小的那些输入，去除起始不足的周围词\n",
    "batch_and_labels = [(x,y) for x,y in batch_and_labels if len(x)==2*windows_size]\n",
    "\n",
    "# extract batch and labels\n",
    "batch, labels = [list(x) for x in zip(*batch_and_labels)]\n",
    "batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.生成cbow模型的批量数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data(sentences, batch_size, windows_size):\n",
    "    # Fill up data batch\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    while len(batch_data) < batch_size:\n",
    "        batch, labels = [], []\n",
    "        # select random sentence to start\n",
    "        rand_sentence = np.random.choice(sentences)\n",
    "        # Generate consecutive windows to look at\n",
    "        window_sequences = [rand_sentence[max((ix - windows_size), 0):(ix + windows_size + 1)] for ix, x in\n",
    "                            enumerate(rand_sentence)]\n",
    "        # 选择中心词\n",
    "        # Denote which element of each window is the center word of interest\n",
    "        label_indices = [ix if ix < windows_size else windows_size for ix, x in enumerate(window_sequences)]\n",
    "\n",
    "        # 这里和skip-gram不一样，sk是(中心词，周围词)，而cbow是（[周围词..]，中心词）\n",
    "        # Pull out center word of interest for each window and create a tuple for each window\n",
    "        batch_and_labels = [(x[:y] + x[(y + 1):], x[y]) for x, y in zip(window_sequences, label_indices)]\n",
    "        # Make it in to a big list of tuples (target word, surrounding word)\n",
    "\n",
    "        # 此时的x输入是Windows_size大小的周围词，y是中心词,，所以要选择周围词大小等于两倍窗口大小的那些输入，去除起始不足的周围词\n",
    "        batch_and_labels = [(x,y) for x,y in batch_and_labels if len(x)==2*windows_size]\n",
    "        \n",
    "        # extract batch and labels\n",
    "        # 比书里的例子中多加了一个判断，因为只筛选了长度>2的句子，而这里的输入要求x大小是2*windows_size\n",
    "        # 如果句子长度小于2*windows_size，则batch_and_labels选择出来是空列表，下一行会出错，所以要加一个判断\n",
    "        if batch_and_labels:\n",
    "            batch, labels = [list(x) for x in zip(*batch_and_labels)]\n",
    "        \n",
    "        batch_data.extend(batch[:batch_size])\n",
    "        label_data.extend(labels[:batch_size])\n",
    "        \n",
    "    # Trim batch and label at the end\n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "\n",
    "    # Convert to numpy array\n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array([label_data]))\n",
    "\n",
    "    return (batch_data, label_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.模型参数初始化\n",
    "#### 6.1初始化变量和函数\n",
    "声明嵌套函数，声明占位符和嵌套查找函数，声明NCE损失的w和b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_2:0' shape=(50, 4) dtype=int32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n",
    "\n",
    "# x是中心词的左右windows_size大小的单词，所以输入大小是2*windows_size\n",
    "x_inputs = tf.placeholder(tf.int32,shape=[batch_size, 2*windows_size])\n",
    "y_target = tf.placeholder(tf.int32,shape=[batch_size,1])\n",
    "valid_dataset = tf.constant(valid_example,dtype=tf.int32)\n",
    "\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size,embedding_size],stddev = 1.0/np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "x_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2处理单词嵌套\n",
    "输入通过embed循环将窗口大小的单词嵌套加在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_8:0' shape=(50, 200) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = tf.zeros([batch_size,embedding_size])\n",
    "for element in range(2*windows_size):\n",
    "    embed += tf.nn.embedding_lookup(embeddings,x_inputs[:,element])\n",
    "embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3定义NCE损失，声明优化器函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights,nce_biases,y_target,embed,num_sampled,vocabulary_size))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1模型变量初始化，定义词向量保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Program Files\\program data\\envs\\py35-tf\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "saver = tf.train.Saver({\"embeddings\":embeddings})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch100:12.670909881591797\n",
      "loss at epoch200:11.690191268920898\n",
      "loss at epoch300:21.26810073852539\n",
      "loss at epoch400:19.435401916503906\n",
      "loss at epoch500:27.821025848388672\n",
      "Dictionary and embeddings saved.\n",
      "loss at epoch600:2.5985732078552246\n",
      "loss at epoch700:22.63739776611328\n",
      "loss at epoch800:19.126953125\n",
      "loss at epoch900:10.38302230834961\n",
      "loss at epoch1000:14.826190948486328\n",
      "Dictionary and embeddings saved.\n"
     ]
    }
   ],
   "source": [
    "loss_vec,loss_x_vec = [],[]\n",
    "for i in range(epochs):   \n",
    "    \n",
    "    batch_input,batch_labels = generate_batch_data(text_data,batch_size,windows_size)\n",
    "\n",
    "    feed_dict = {x_inputs:batch_input,y_target:batch_labels}\n",
    "    sess.run(optimizer,feed_dict)\n",
    "    \n",
    "    if (i+1)%100 ==0:\n",
    "        loss_val = sess.run(loss,feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print(\"loss at epoch{}:{}\".format(i+1,loss_val))\n",
    "        \n",
    "    # 保存词表和模型\n",
    "    if (i+1)%500 == 0:\n",
    "        \n",
    "        with open(\"vocab\\\\movie_vocab.pkl\",\"wb\") as f:\n",
    "            pickle.dump(word_dictionary,f)\n",
    "        \n",
    "        #  Parent directory of cbow_movie_embeddings.ckpt doesn't exist, can't save.\n",
    "        # 出现了这个错误，于是给ckpt文件加了一个文件夹\n",
    "        save_path = saver.save(sess,os.path.join(\"models\",\"cbow_movie_embeddings.ckpt\"))\n",
    "        \n",
    "        print(\"Dictionary and embeddings saved.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来看一下.pkl文件保存了啥-->就是word_dictionary本身那个词汇赋值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'form': 515,\n",
       " 'del': 1241,\n",
       " 'brush': 8498,\n",
       " 'seemed': 2033,\n",
       " 'subjects': 998,\n",
       " 'planet': 1940,\n",
       " 'threshold': 8499,\n",
       " 'italian': 1289,\n",
       " 'coldhearted': 9296,\n",
       " 'speak': 2906,\n",
       " 'tract': 6935,\n",
       " 'appreciated': 3969,\n",
       " 'lacks': 237,\n",
       " 'generated': 5294,\n",
       " 'selfimportant': 4327,\n",
       " 'range': 3970,\n",
       " 'jerking': 5295,\n",
       " 'maybe': 574,\n",
       " 'ideological': 8501,\n",
       " 'grownups': 3093,\n",
       " 'bleak': 2289,\n",
       " 'boisterous': 3629,\n",
       " 'succumbing': 6953,\n",
       " 'idealistic': 5296,\n",
       " 'exploits': 4834,\n",
       " 'punk': 4835,\n",
       " 'enthusiastically': 6936,\n",
       " 'conquer': 5445,\n",
       " 'curmudgeon': 4836,\n",
       " 'performer': 6095,\n",
       " 'depicting': 6937,\n",
       " 'years': 93,\n",
       " 'murky': 3360,\n",
       " 'necessity': 6938,\n",
       " 'construction': 3094,\n",
       " 'formulaic': 742,\n",
       " 'turn': 397,\n",
       " 'whine': 5944,\n",
       " 'extremely': 755,\n",
       " 'spielberg': 999,\n",
       " 'dustin': 7275,\n",
       " 'issue': 1698,\n",
       " 'jagged': 6939,\n",
       " 'dynamite': 8505,\n",
       " 'mildmannered': 8506,\n",
       " 'fleming': 6940,\n",
       " 'deliciously': 4758,\n",
       " 'moments': 110,\n",
       " 'atmospheric': 4328,\n",
       " 'unfocused': 1941,\n",
       " 'enriched': 6096,\n",
       " 'oftenfunny': 8509,\n",
       " 'cia': 3630,\n",
       " 'payback': 9083,\n",
       " 'charm': 199,\n",
       " 'models': 8512,\n",
       " 'frenzy': 5299,\n",
       " 'dean': 5321,\n",
       " 'enchanting': 4837,\n",
       " 'dilutes': 8513,\n",
       " 'chabrol': 3631,\n",
       " 'ring': 1194,\n",
       " 'sweetly': 2934,\n",
       " 'igloo': 6942,\n",
       " 'fest': 3632,\n",
       " 'warmth': 938,\n",
       " 'crack': 3633,\n",
       " 'nolan': 5300,\n",
       " 'martha': 1577,\n",
       " 'eyeballs': 8190,\n",
       " 'connoisseurs': 5301,\n",
       " 'ape': 6943,\n",
       " 'openminded': 6099,\n",
       " 'attitude': 1942,\n",
       " 'hopes': 3361,\n",
       " 'improved': 4022,\n",
       " 'crises': 8516,\n",
       " 'democracy': 6944,\n",
       " 'immediacy': 5302,\n",
       " 'studios': 3095,\n",
       " 'uniqueness': 6945,\n",
       " 'worm': 9939,\n",
       " 'blockbusters': 5169,\n",
       " 'silver': 3111,\n",
       " 'butter': 7591,\n",
       " 'unsettling': 736,\n",
       " 'wryly': 6100,\n",
       " 'timing': 1766,\n",
       " 'hush': 6946,\n",
       " 'sting': 3362,\n",
       " 'fills': 3363,\n",
       " 'becomes': 233,\n",
       " 'lump': 3875,\n",
       " 'frowns': 7008,\n",
       " 'purely': 2895,\n",
       " 'disservice': 7013,\n",
       " 'reflective': 3635,\n",
       " 'horizons': 8519,\n",
       " 'lessons': 3096,\n",
       " 'tell': 433,\n",
       " 'things': 119,\n",
       " 'cover': 2546,\n",
       " 'pena': 3636,\n",
       " 'flying': 5305,\n",
       " 'spaceship': 8520,\n",
       " 'scotland': 3637,\n",
       " 'warfare': 4839,\n",
       " 'flounders': 4330,\n",
       " 'perfectly': 543,\n",
       " 'performs': 6948,\n",
       " 'american': 114,\n",
       " 'joy': 1436,\n",
       " 'lists': 7802,\n",
       " 'surrounded': 6102,\n",
       " 'everyone': 335,\n",
       " 'unhappy': 4331,\n",
       " 'magic': 618,\n",
       " 'disturb': 6950,\n",
       " 'flies': 6951,\n",
       " 'majority': 4332,\n",
       " 'moore': 1767,\n",
       " 'verve': 2146,\n",
       " 'sleek': 5306,\n",
       " 'confusing': 1684,\n",
       " 'fool': 3364,\n",
       " 'villeneuve': 4333,\n",
       " 'zippy': 5047,\n",
       " 'transforms': 4323,\n",
       " 'exciting': 671,\n",
       " 'jarring': 3973,\n",
       " 'traditions': 5217,\n",
       " 'aloft': 6952,\n",
       " 'loves': 1516,\n",
       " 'waking': 3974,\n",
       " 'operatic': 6103,\n",
       " 'alpha': 9100,\n",
       " 'image': 2147,\n",
       " 'sword': 8522,\n",
       " 'sense': 69,\n",
       " 'preaching': 8523,\n",
       " 'exemplary': 8503,\n",
       " 'superior': 1340,\n",
       " 'certain': 630,\n",
       " 'exact': 5372,\n",
       " 'bourne': 2035,\n",
       " 'sodden': 8101,\n",
       " 'pastry': 8995,\n",
       " 'singing': 2697,\n",
       " 'poses': 8189,\n",
       " 'slip': 2166,\n",
       " 'odds': 2148,\n",
       " 'selfdeprecating': 3097,\n",
       " 'drama': 39,\n",
       " 'brutality': 6954,\n",
       " 'echoes': 7076,\n",
       " 'considered': 2761,\n",
       " 'unendurable': 6104,\n",
       " 'bland': 575,\n",
       " 'machine': 1237,\n",
       " 'demographically': 7082,\n",
       " 'offer': 576,\n",
       " 'murphy': 1000,\n",
       " 'spare': 1578,\n",
       " 'advertised': 6105,\n",
       " 'backed': 5308,\n",
       " 'script': 74,\n",
       " 'streamed': 6956,\n",
       " 'asphalt': 6957,\n",
       " 'mercy': 6107,\n",
       " 'rewarding': 1907,\n",
       " 'frustration': 3098,\n",
       " 'added': 3638,\n",
       " 'enigmatic': 3099,\n",
       " 'drugs': 1853,\n",
       " 'low': 639,\n",
       " 'follows': 2069,\n",
       " 'extensive': 5309,\n",
       " 'crudup': 4335,\n",
       " 'knows': 529,\n",
       " 'performing': 5310,\n",
       " 'retooled': 6958,\n",
       " 'candor': 8009,\n",
       " 'comic': 213,\n",
       " 'convey': 2149,\n",
       " 'slambang': 8998,\n",
       " 'perry': 6085,\n",
       " 'sequel': 430,\n",
       " 'standard': 892,\n",
       " 'genres': 1854,\n",
       " 'glorious': 3100,\n",
       " 'rock': 544,\n",
       " 'real': 81,\n",
       " 'produce': 2170,\n",
       " 'reaches': 3640,\n",
       " 'musty': 3641,\n",
       " 'todays': 3102,\n",
       " 'nevertheless': 2087,\n",
       " 'blunt': 4840,\n",
       " 'crassly': 6959,\n",
       " 'relating': 6790,\n",
       " 'constructs': 6379,\n",
       " 'candy': 2098,\n",
       " 'luster': 5313,\n",
       " 'contradiction': 5314,\n",
       " 'noyce': 3103,\n",
       " 'mocking': 7228,\n",
       " 'withstand': 6961,\n",
       " 'lurking': 5399,\n",
       " 'putting': 3367,\n",
       " 'receiving': 8504,\n",
       " 'themed': 7151,\n",
       " 'nerdy': 8530,\n",
       " 'ambition': 1768,\n",
       " 'mama': 6962,\n",
       " 'overkill': 3642,\n",
       " 'crushingly': 5404,\n",
       " 'pack': 1943,\n",
       " 'mike': 1342,\n",
       " 'detailed': 1579,\n",
       " 'naipauls': 8531,\n",
       " 'footnote': 4337,\n",
       " 'least': 144,\n",
       " 'forever': 3975,\n",
       " 'underdogs': 6964,\n",
       " 'spiced': 5266,\n",
       " 'conversation': 3643,\n",
       " 'hint': 3536,\n",
       " 'ozpetek': 3676,\n",
       " 'words': 672,\n",
       " 'dud': 2548,\n",
       " 'guide': 5813,\n",
       " 'hoffmans': 2413,\n",
       " 'biblical': 8533,\n",
       " 'oldschool': 4842,\n",
       " 'scribe': 6992,\n",
       " 'presenting': 6109,\n",
       " 'airless': 5317,\n",
       " 'evenhanded': 6966,\n",
       " 'opposite': 5966,\n",
       " 'football': 4843,\n",
       " 'truck': 7003,\n",
       " 'across': 654,\n",
       " 'actioner': 8536,\n",
       " 'goodhearted': 4338,\n",
       " 'stalking': 6967,\n",
       " 'rotting': 8537,\n",
       " 'exaggerated': 3369,\n",
       " 'lessen': 9883,\n",
       " 'speaks': 2036,\n",
       " 'gone': 463,\n",
       " 'abroad': 6969,\n",
       " 'worry': 4229,\n",
       " 'four': 516,\n",
       " 'approaching': 6619,\n",
       " 'breath': 2150,\n",
       " 'bound': 4844,\n",
       " 'sway': 8538,\n",
       " 'kung': 1855,\n",
       " 'speeds': 6971,\n",
       " 'local': 3171,\n",
       " 'supremely': 4581,\n",
       " 'aplenty': 8539,\n",
       " 'onenote': 8265,\n",
       " 'celebrates': 2037,\n",
       " 'material': 134,\n",
       " 'penchant': 8540,\n",
       " 'separate': 3976,\n",
       " 'worldclass': 5318,\n",
       " 'gyllenhaal': 3977,\n",
       " 'letters': 8541,\n",
       " 'deeds': 2736,\n",
       " 'crisp': 2291,\n",
       " 'ears': 3978,\n",
       " 'woody': 1494,\n",
       " 'supporting': 1629,\n",
       " 'enterprise': 1851,\n",
       " 'insight': 486,\n",
       " 'spread': 4763,\n",
       " 'little': 13,\n",
       " 'enormously': 2911,\n",
       " 'attraction': 1113,\n",
       " 'still': 46,\n",
       " 'pleasurable': 3035,\n",
       " 'bombing': 8542,\n",
       " 'surroundings': 5438,\n",
       " 'critique': 5319,\n",
       " 'babysitter': 8543,\n",
       " 'universal': 1057,\n",
       " 'bile': 9052,\n",
       " 'alluring': 4339,\n",
       " 'lured': 9613,\n",
       " 'selfesteem': 5320,\n",
       " 'rural': 4340,\n",
       " 'timid': 4341,\n",
       " 'selfhatred': 8173,\n",
       " 'practice': 5215,\n",
       " 'somehow': 844,\n",
       " 'entire': 504,\n",
       " 'imply': 8544,\n",
       " 'conflict': 920,\n",
       " 'sardonic': 3539,\n",
       " 'thesis': 3979,\n",
       " 'unrelenting': 5322,\n",
       " 'ten': 3143,\n",
       " 'remake': 530,\n",
       " 'rifkin': 8545,\n",
       " 'bomb': 4343,\n",
       " 'pork': 7706,\n",
       " 'truths': 3888,\n",
       " 'men': 259,\n",
       " 'thrives': 6975,\n",
       " 'nonsensical': 2739,\n",
       " 'america': 1022,\n",
       " 'stripped': 5324,\n",
       " 'collect': 8514,\n",
       " 'tissues': 5325,\n",
       " 'hero': 517,\n",
       " 'examines': 2699,\n",
       " 'loopiness': 6977,\n",
       " 'spectacularly': 4154,\n",
       " 'shyamalans': 4344,\n",
       " 'rrated': 9002,\n",
       " 'bollywood': 4291,\n",
       " 'brosnans': 8546,\n",
       " 'miraculous': 8547,\n",
       " 'trailer': 2521,\n",
       " 'vaguely': 3106,\n",
       " 'expertly': 2913,\n",
       " 'lifted': 8548,\n",
       " 'tediously': 4846,\n",
       " 'loved': 1238,\n",
       " 'crippled': 6897,\n",
       " 'problemas': 6979,\n",
       " 'oozes': 6980,\n",
       " 'sorvino': 3644,\n",
       " 'discourse': 5327,\n",
       " 'shape': 3645,\n",
       " 'friday': 1343,\n",
       " 'special': 162,\n",
       " 'coherence': 4345,\n",
       " 'directors': 424,\n",
       " 'michel': 3107,\n",
       " 'fake': 2152,\n",
       " 'climactic': 3371,\n",
       " 'proof': 3372,\n",
       " 'absorbs': 8550,\n",
       " 'demmes': 5475,\n",
       " 'tricky': 6223,\n",
       " 'figure': 1058,\n",
       " 'tops': 6983,\n",
       " 'broke': 8551,\n",
       " 'solondzs': 3646,\n",
       " 'inspired': 939,\n",
       " 'squarely': 3373,\n",
       " 'heartstrings': 8552,\n",
       " 'bang': 5328,\n",
       " 'slightly': 716,\n",
       " 'youarethere': 9537,\n",
       " 'reverence': 5483,\n",
       " 'less': 76,\n",
       " 'wildly': 1023,\n",
       " 'affair': 1024,\n",
       " 'japan': 3374,\n",
       " 'humor': 66,\n",
       " 'concoction': 3647,\n",
       " 'scripting': 5042,\n",
       " 'anomie': 3648,\n",
       " 'sure': 297,\n",
       " 'concocted': 7314,\n",
       " 'accomplishment': 6115,\n",
       " 'reggio': 3980,\n",
       " 'flaw': 6116,\n",
       " 'soulful': 2834,\n",
       " 'volatile': 2700,\n",
       " 'petes': 8554,\n",
       " 'bar': 3154,\n",
       " 'processor': 7105,\n",
       " 'passage': 8556,\n",
       " 'envelope': 8557,\n",
       " 'needy': 8558,\n",
       " 'lends': 2551,\n",
       " 'sufficient': 6117,\n",
       " 'legged': 2399,\n",
       " 'last': 160,\n",
       " 'scarier': 6986,\n",
       " 'actually': 198,\n",
       " 'coming': 631,\n",
       " 'remarkably': 971,\n",
       " 'flat': 421,\n",
       " 'byzantine': 6987,\n",
       " 'jealousy': 1856,\n",
       " 'patchy': 6988,\n",
       " 'means': 926,\n",
       " 'focusing': 4347,\n",
       " 'flatly': 8737,\n",
       " 'ones': 469,\n",
       " 'fundamentals': 3981,\n",
       " 'hotel': 2701,\n",
       " 'mostly': 268,\n",
       " 'yvans': 8559,\n",
       " 'expecting': 4847,\n",
       " 'overwhelm': 8472,\n",
       " 'ye': 8560,\n",
       " 'hospital': 3982,\n",
       " 'consciousness': 2292,\n",
       " 'change': 545,\n",
       " 'relationships': 717,\n",
       " 'por': 3075,\n",
       " 'proper': 3983,\n",
       " 'hardpressed': 6990,\n",
       " 'overthetop': 1512,\n",
       " 'talky': 2702,\n",
       " 'anticipated': 6118,\n",
       " 'lived': 1857,\n",
       " 'suspension': 5331,\n",
       " 'imax': 868,\n",
       " 'cop': 2153,\n",
       " 'hackneyed': 1195,\n",
       " 'feels': 73,\n",
       " 'weaving': 6991,\n",
       " 'kane': 6119,\n",
       " 'pummel': 8562,\n",
       " 'slightest': 4848,\n",
       " 'pedestrian': 2703,\n",
       " 'susan': 3109,\n",
       " 'mercilessly': 8563,\n",
       " 'newcomers': 5955,\n",
       " 'wondered': 5333,\n",
       " 'demonstration': 6121,\n",
       " 'cranky': 7364,\n",
       " 'examples': 5334,\n",
       " 'pleasures': 1114,\n",
       " 'jovial': 8565,\n",
       " 'rabbitproof': 3883,\n",
       " 'unbelievable': 3110,\n",
       " 'firmly': 3117,\n",
       " 'impressed': 3375,\n",
       " 'invention': 3236,\n",
       " 'forcefully': 4849,\n",
       " 'winter': 4348,\n",
       " 'showtime': 1755,\n",
       " 'terrifically': 6993,\n",
       " 'somewhere': 893,\n",
       " 'put': 265,\n",
       " 'diggs': 3651,\n",
       " 'robots': 7862,\n",
       " 'janice': 5113,\n",
       " 'handling': 6122,\n",
       " 'theology': 8569,\n",
       " 'diva': 3634,\n",
       " 'changing': 1859,\n",
       " 'without': 52,\n",
       " 'entity': 6995,\n",
       " 'prime': 2838,\n",
       " 'says': 2154,\n",
       " 'affluent': 8571,\n",
       " 'improvise': 8572,\n",
       " 'outrageous': 1290,\n",
       " 'highspirited': 7386,\n",
       " 'narc': 1944,\n",
       " 'underdeveloped': 8573,\n",
       " 'remembering': 3687,\n",
       " 'towers': 6996,\n",
       " 'mood': 518,\n",
       " 'box': 2704,\n",
       " 'jfk': 6998,\n",
       " 'fall': 1291,\n",
       " 'clashing': 3984,\n",
       " 'screams': 4850,\n",
       " 'camouflage': 4782,\n",
       " 'weep': 4757,\n",
       " 'parentchild': 6999,\n",
       " 'rapidfire': 7000,\n",
       " 'remotely': 2914,\n",
       " 'result': 528,\n",
       " 'threatened': 7001,\n",
       " 'absurd': 1860,\n",
       " 'heads': 2553,\n",
       " 'perilously': 6347,\n",
       " 'emphasized': 7002,\n",
       " 'moviegoing': 1707,\n",
       " 'shout': 4349,\n",
       " 'lifeless': 1861,\n",
       " 'chase': 1122,\n",
       " 'purposeful': 7004,\n",
       " 'adage': 5335,\n",
       " 'heres': 1385,\n",
       " 'roads': 5336,\n",
       " 'uneasily': 5337,\n",
       " 'decade': 2705,\n",
       " 'handicapped': 8576,\n",
       " 'language': 1769,\n",
       " 'naked': 4350,\n",
       " 'shaken': 7761,\n",
       " 'dridi': 8578,\n",
       " 'crushes': 8579,\n",
       " 'survivors': 4351,\n",
       " 'norm': 8580,\n",
       " 'comprehension': 5338,\n",
       " 'winner': 1700,\n",
       " 'intact': 4851,\n",
       " 'fraternity': 7435,\n",
       " 'creativity': 1580,\n",
       " 'cherish': 3112,\n",
       " 'moment': 513,\n",
       " 'canon': 3967,\n",
       " 'tracking': 7006,\n",
       " 'realizing': 5670,\n",
       " 'eastwood': 3376,\n",
       " 'architectural': 8581,\n",
       " 'sandlers': 2155,\n",
       " 'wiser': 5433,\n",
       " 'strain': 2586,\n",
       " 'plotline': 4352,\n",
       " 'topics': 5340,\n",
       " 'away': 172,\n",
       " 'belt': 2996,\n",
       " 'images': 387,\n",
       " 'blown': 3752,\n",
       " 'heated': 6124,\n",
       " 'counts': 6641,\n",
       " 'laudable': 6125,\n",
       " 'caruso': 6506,\n",
       " 'earns': 3986,\n",
       " 'odor': 7009,\n",
       " 'sledgehammer': 7010,\n",
       " 'poorly': 1066,\n",
       " 'lyrical': 1701,\n",
       " 'puppet': 6363,\n",
       " 'morettis': 7011,\n",
       " 'team': 1309,\n",
       " 'fluff': 2156,\n",
       " 'model': 8582,\n",
       " 'bravery': 4353,\n",
       " 'substitutes': 6127,\n",
       " 'reno': 1292,\n",
       " 'equilibrium': 3538,\n",
       " 'probably': 225,\n",
       " 'poignancy': 1648,\n",
       " 'delicious': 2157,\n",
       " 'press': 4354,\n",
       " 'insanely': 3377,\n",
       " 'room': 983,\n",
       " 'unprecedented': 7479,\n",
       " 'likeable': 3988,\n",
       " 'goal': 3319,\n",
       " 'inventiveness': 6130,\n",
       " 'politics': 869,\n",
       " 'mugs': 7014,\n",
       " 'bravo': 8584,\n",
       " 'probing': 4355,\n",
       " 'sabotaged': 5586,\n",
       " 'johnson': 8585,\n",
       " 'fuhrmans': 8271,\n",
       " 'threehour': 4854,\n",
       " 'west': 2293,\n",
       " 'shoots': 5344,\n",
       " 'sensibilities': 3654,\n",
       " 'offkilter': 6131,\n",
       " 'supersized': 7015,\n",
       " 'robin': 2038,\n",
       " 'doomed': 7016,\n",
       " 'criticism': 6132,\n",
       " 'drains': 7017,\n",
       " 'implicit': 5345,\n",
       " 'mothers': 3378,\n",
       " 'fans': 170,\n",
       " 'mcculloch': 8587,\n",
       " 'trenchant': 4511,\n",
       " 'camerawork': 5304,\n",
       " 'energetic': 894,\n",
       " 'alienate': 6133,\n",
       " 'halfway': 1945,\n",
       " 'descends': 5346,\n",
       " 'squander': 7018,\n",
       " 'knowing': 1631,\n",
       " 'cornball': 7019,\n",
       " 'scoring': 8588,\n",
       " 'tiresome': 1240,\n",
       " 'strongly': 3309,\n",
       " 'perform': 7020,\n",
       " 'field': 3180,\n",
       " 'refreshed': 8589,\n",
       " 'urgency': 2400,\n",
       " 'indistinct': 9860,\n",
       " 'adapted': 3972,\n",
       " 'stamina': 7352,\n",
       " 'plummer': 6366,\n",
       " 'foot': 5597,\n",
       " 'straighttovideo': 7945,\n",
       " 'tootsie': 5933,\n",
       " 'sane': 3115,\n",
       " 'druggy': 8590,\n",
       " 'scattershot': 5347,\n",
       " 'believe': 336,\n",
       " 'adaptation': 546,\n",
       " 'listen': 2554,\n",
       " 'times': 127,\n",
       " 'scorpion': 5221,\n",
       " 'selfimportance': 8591,\n",
       " 'pretend': 5348,\n",
       " 'civil': 6135,\n",
       " 'audacious': 2706,\n",
       " 'paced': 1138,\n",
       " 'strays': 8592,\n",
       " 'unbelievably': 6136,\n",
       " 'hitler': 7023,\n",
       " 'overlooked': 5349,\n",
       " 'remakes': 8593,\n",
       " 'boldly': 3259,\n",
       " 'type': 1344,\n",
       " 'undone': 2908,\n",
       " 'ivan': 7024,\n",
       " 'continuation': 8596,\n",
       " 'open': 1059,\n",
       " 'critic': 5350,\n",
       " 'flowers': 4121,\n",
       " 'slick': 1386,\n",
       " 'charmer': 2707,\n",
       " 'child': 1514,\n",
       " 'resemblance': 5352,\n",
       " 'schaeffers': 7727,\n",
       " 'crew': 2708,\n",
       " 'valid': 7027,\n",
       " 'jackass': 4357,\n",
       " 'per': 6507,\n",
       " 'refuses': 2158,\n",
       " 'faithful': 1449,\n",
       " 'nervy': 3379,\n",
       " 'gosford': 6101,\n",
       " 'beijing': 4855,\n",
       " 'am闁榠e': 7030,\n",
       " 'motivation': 5353,\n",
       " 'monte': 4791,\n",
       " 'manipulation': 4856,\n",
       " 'fathers': 3380,\n",
       " 'shakespeares': 3965,\n",
       " 'shared': 5354,\n",
       " 'ago': 718,\n",
       " 'brits': 5514,\n",
       " 'vincent': 2401,\n",
       " 'toes': 4857,\n",
       " 'abyss': 8602,\n",
       " 'grating': 3078,\n",
       " 'exploring': 3381,\n",
       " 'worrying': 5441,\n",
       " 'soulless': 2670,\n",
       " 'contemporaries': 8605,\n",
       " 'diversion': 1862,\n",
       " 'n': 2555,\n",
       " 'carrey': 8606,\n",
       " 'current': 1702,\n",
       " 'characters': 11,\n",
       " 'fine': 288,\n",
       " 'absurdities': 4267,\n",
       " 'san': 5628,\n",
       " 'leonine': 7587,\n",
       " 'juicy': 3990,\n",
       " 'lowwattage': 8526,\n",
       " 'coincidence': 6351,\n",
       " 'suggesting': 6141,\n",
       " 'uma': 3276,\n",
       " 'recycling': 4759,\n",
       " 'rudimentary': 7035,\n",
       " 'languid': 7031,\n",
       " 'entirely': 479,\n",
       " 'eager': 3382,\n",
       " 'proves': 368,\n",
       " 'award': 3383,\n",
       " 'underwritten': 4859,\n",
       " 'man': 131,\n",
       " 'unless': 1206,\n",
       " 'blackboards': 7032,\n",
       " 'torturing': 9313,\n",
       " 'win': 1115,\n",
       " 'tremendously': 7034,\n",
       " 'anyones': 8608,\n",
       " 'scorseses': 2709,\n",
       " 'direction': 188,\n",
       " 'degrees': 7036,\n",
       " 'triumphantly': 7037,\n",
       " 'terrorists': 9309,\n",
       " 'nuance': 2710,\n",
       " 'going': 148,\n",
       " 'carved': 6142,\n",
       " 'encounters': 4359,\n",
       " 'purpose': 1001,\n",
       " 'oliveira': 7039,\n",
       " 'miserable': 2457,\n",
       " 'quiet': 531,\n",
       " 'harvard': 1450,\n",
       " 'satisfying': 382,\n",
       " 'standards': 1293,\n",
       " 'familial': 3384,\n",
       " 'distant': 4254,\n",
       " 'source': 1451,\n",
       " 'playing': 674,\n",
       " 'white': 912,\n",
       " 'producers': 2763,\n",
       " 'hoped': 2547,\n",
       " 'wars': 1581,\n",
       " 'emphasizes': 2918,\n",
       " 'fields': 4361,\n",
       " 'exterior': 5307,\n",
       " 'ghoulish': 8613,\n",
       " 'next': 279,\n",
       " 'coincidences': 7042,\n",
       " 'walls': 2711,\n",
       " 'given': 402,\n",
       " 'treatment': 1025,\n",
       " 'racy': 5662,\n",
       " 'problems': 757,\n",
       " 'overlook': 8183,\n",
       " 'destined': 2556,\n",
       " 'sunday': 4540,\n",
       " 'pageturner': 7044,\n",
       " 'consistent': 3656,\n",
       " 'taken': 689,\n",
       " 'sleeve': 3385,\n",
       " 'austerity': 5358,\n",
       " 'odyssey': 3657,\n",
       " 'bawdy': 7045,\n",
       " 'penance': 7930,\n",
       " 'workinprogress': 6963,\n",
       " 'reacting': 8619,\n",
       " 'physique': 7046,\n",
       " 'longer': 1090,\n",
       " 'canadian': 8620,\n",
       " 'passing': 3386,\n",
       " 'brains': 3928,\n",
       " 'werent': 2161,\n",
       " 'forced': 865,\n",
       " 'reach': 1294,\n",
       " 'ongoing': 5359,\n",
       " 'selfabsorbed': 5360,\n",
       " 'tensions': 4860,\n",
       " 'practically': 2402,\n",
       " 'bettany': 3992,\n",
       " 'fused': 6146,\n",
       " 'sheds': 7049,\n",
       " 'memento': 5362,\n",
       " 'romanek': 4861,\n",
       " 'distracting': 5363,\n",
       " 'fulfilling': 5580,\n",
       " 'acquainted': 7050,\n",
       " 'haul': 8469,\n",
       " 'fist': 9925,\n",
       " 'unexamined': 7051,\n",
       " 'tactics': 5668,\n",
       " 'headache': 8566,\n",
       " 'mattei': 6147,\n",
       " 'doo': 5364,\n",
       " 'lawrences': 5365,\n",
       " 'laughter': 1517,\n",
       " 'careers': 5366,\n",
       " 'jaded': 4551,\n",
       " 'honorable': 8063,\n",
       " 'id': 1080,\n",
       " 'hollywood': 99,\n",
       " 'autopilot': 5694,\n",
       " 'adequate': 3217,\n",
       " 'sus': 8624,\n",
       " 'writers': 1295,\n",
       " 'pressed': 4795,\n",
       " 'rowdy': 3994,\n",
       " 'intelligently': 5367,\n",
       " 'impulse': 5368,\n",
       " 'yields': 8625,\n",
       " 'lo': 2403,\n",
       " 'gives': 146,\n",
       " 'primarily': 2919,\n",
       " 'stultifyingly': 6149,\n",
       " 'unfortunate': 3659,\n",
       " 'portray': 3222,\n",
       " 'tim': 5239,\n",
       " 'tastelessness': 4862,\n",
       " 'wilde': 4863,\n",
       " 'pleased': 7053,\n",
       " 'features': 1387,\n",
       " 'welcome': 607,\n",
       " 'detriment': 7054,\n",
       " 'messages': 1582,\n",
       " 'experimental': 4362,\n",
       " 'bart': 7055,\n",
       " 'marching': 3995,\n",
       " 'flashback': 7056,\n",
       " 'freshness': 3227,\n",
       " 'savor': 6473,\n",
       " 'architecture': 6150,\n",
       " 'thurman': 8627,\n",
       " 'give': 155,\n",
       " 'familiarity': 3387,\n",
       " 'grinder': 7057,\n",
       " 'insane': 6151,\n",
       " 'terrorist': 3229,\n",
       " 'bitter': 1633,\n",
       " 'wounds': 6152,\n",
       " 'finesse': 6153,\n",
       " 'sanctimony': 4363,\n",
       " 'sadistic': 4364,\n",
       " 'bluster': 5877,\n",
       " 'rut': 5369,\n",
       " 'paved': 8629,\n",
       " 'noisy': 6154,\n",
       " 'freshfaced': 8630,\n",
       " 'loveable': 7059,\n",
       " 'madcap': 6155,\n",
       " 'flashbacks': 3675,\n",
       " 'shamelessly': 2404,\n",
       " 'oddest': 4864,\n",
       " 'sensationalism': 7061,\n",
       " 'wary': 8631,\n",
       " 'asia': 8632,\n",
       " 'tripe': 7737,\n",
       " 'actions': 3996,\n",
       " 'shame': 1158,\n",
       " 'whatever': 895,\n",
       " 'gloriously': 3119,\n",
       " 'subject': 156,\n",
       " 'publishing': 7748,\n",
       " 'distinguished': 3661,\n",
       " 'managed': 2162,\n",
       " 'topic': 2163,\n",
       " 'grown': 4866,\n",
       " 'eats': 9875,\n",
       " 'dime': 7063,\n",
       " 'return': 1243,\n",
       " 'childhood': 2089,\n",
       " 'plodding': 1590,\n",
       " 'storylines': 4867,\n",
       " 'story': 5,\n",
       " 'abbreviated': 7064,\n",
       " 'span': 6157,\n",
       " 'tradition': 2040,\n",
       " 'apenas': 7770,\n",
       " 'songs': 1204,\n",
       " 'endings': 6227,\n",
       " 'prop': 7067,\n",
       " 'tame': 6158,\n",
       " 'lowtech': 7068,\n",
       " 'promises': 2164,\n",
       " 'prose': 3663,\n",
       " 'gut': 8636,\n",
       " 'examination': 641,\n",
       " 'este': 6159,\n",
       " 'bales': 7069,\n",
       " 'gonna': 7070,\n",
       " 'culkin': 3998,\n",
       " 'thematic': 2165,\n",
       " 'head': 416,\n",
       " 'scenic': 4600,\n",
       " 'subtlest': 8639,\n",
       " 'made': 37,\n",
       " 'layers': 3365,\n",
       " 'amalgam': 4373,\n",
       " 'vera': 7312,\n",
       " 'gamut': 7071,\n",
       " 'view': 619,\n",
       " 'speed': 6160,\n",
       " 'popular': 2406,\n",
       " 'tribute': 1244,\n",
       " 'broken': 1910,\n",
       " 'awkwardness': 4870,\n",
       " 'sweetandsour': 8642,\n",
       " 'relentless': 1771,\n",
       " 'kind': 67,\n",
       " 'correct': 4871,\n",
       " 'called': 870,\n",
       " 'rigor': 5373,\n",
       " 'prisoners': 8643,\n",
       " 'acted': 472,\n",
       " 'shines': 2407,\n",
       " 'humble': 5374,\n",
       " 'extreme': 796,\n",
       " 'lord': 4872,\n",
       " 'bucks': 3388,\n",
       " 'pathetic': 1772,\n",
       " 'sucking': 3999,\n",
       " 'lot': 113,\n",
       " 'reviews': 7075,\n",
       " 'adams': 2114,\n",
       " 'seats': 3120,\n",
       " 'modern': 305,\n",
       " 'tangled': 5375,\n",
       " 'gaytons': 5376,\n",
       " 'benefits': 4000,\n",
       " 'sweetest': 4197,\n",
       " 'eastern': 5379,\n",
       " 'ride': 354,\n",
       " 'epics': 4617,\n",
       " 'spinning': 3121,\n",
       " 'saucy': 5377,\n",
       " 'help': 449,\n",
       " 'variety': 2557,\n",
       " 'spotlight': 5378,\n",
       " 'disgusted': 8645,\n",
       " 'passes': 3389,\n",
       " 'crush': 719,\n",
       " 'timeless': 2920,\n",
       " 'blood': 1159,\n",
       " 'abhorrent': 8646,\n",
       " 'scripts': 1653,\n",
       " 'carveys': 3390,\n",
       " 'credibility': 1757,\n",
       " 'integrated': 7079,\n",
       " 'denzel': 4620,\n",
       " 'sake': 3664,\n",
       " 'spice': 3391,\n",
       " 'unseen': 9558,\n",
       " 'grinds': 7829,\n",
       " 'realization': 4367,\n",
       " 'artistes': 7080,\n",
       " 'recovery': 4625,\n",
       " 'subconscious': 6327,\n",
       " 'talento': 7138,\n",
       " 'unturned': 6164,\n",
       " 'blisteringly': 6165,\n",
       " 'rate': 4368,\n",
       " 'margarita': 6166,\n",
       " 'climate': 6167,\n",
       " 'treated': 5380,\n",
       " 'depressingly': 6638,\n",
       " 'wish': 1002,\n",
       " 'abundantly': 8650,\n",
       " 'collective': 4001,\n",
       " 'sugary': 4369,\n",
       " 'army': 7844,\n",
       " 'arteta': 8651,\n",
       " 'maternal': 9018,\n",
       " 'cloaked': 7083,\n",
       " 'petty': 7851,\n",
       " 'note': 3392,\n",
       " 'beenthere': 7084,\n",
       " 'trees': 5381,\n",
       " 'mouse': 7085,\n",
       " 'resembling': 3665,\n",
       " 'efficient': 6168,\n",
       " 'enjoys': 3393,\n",
       " 'charming': 246,\n",
       " 'bridget': 5382,\n",
       " 'unpretentious': 4002,\n",
       " 'tickets': 6169,\n",
       " 'companys': 7086,\n",
       " 'davies': 6020,\n",
       " 'sensual': 2295,\n",
       " 'national': 1345,\n",
       " 'alltime': 6228,\n",
       " 'pore': 6348,\n",
       " 'prison': 1864,\n",
       " 'mejor': 8001,\n",
       " 'squeeze': 3394,\n",
       " 'roughshod': 7088,\n",
       " 'craig': 4371,\n",
       " 'procession': 4372,\n",
       " 'watchable': 812,\n",
       " 'nails': 4003,\n",
       " 'sorry': 2590,\n",
       " 'light': 359,\n",
       " 'refuse': 6106,\n",
       " 'obnoxious': 2041,\n",
       " 'greens': 8654,\n",
       " 'scarlet': 6170,\n",
       " 'weaponry': 7089,\n",
       " 'tolerate': 6171,\n",
       " 'latin': 3666,\n",
       " 'easily': 374,\n",
       " 'wannabe': 2296,\n",
       " 'oily': 8612,\n",
       " 'superbly': 2167,\n",
       " 'decadent': 3667,\n",
       " 'ron': 6172,\n",
       " 'epicenter': 8656,\n",
       " 'corn': 7092,\n",
       " 'element': 2125,\n",
       " 'interludes': 5383,\n",
       " 'taboo': 7093,\n",
       " 'revealed': 4777,\n",
       " 'hiphop': 1346,\n",
       " 'remain': 2168,\n",
       " 'whiff': 8659,\n",
       " 'including': 1452,\n",
       " 'fussy': 8660,\n",
       " 'unpleasant': 1337,\n",
       " 'crisis': 2297,\n",
       " 'defeated': 6175,\n",
       " 'insurrection': 9487,\n",
       " 'heavily': 2921,\n",
       " 'indie': 1081,\n",
       " 'composition': 4374,\n",
       " 'pryor': 4874,\n",
       " 'let': 973,\n",
       " 'waste': 720,\n",
       " 'disparate': 4410,\n",
       " 'plods': 5009,\n",
       " 'australia': 6176,\n",
       " 'discussion': 3668,\n",
       " 'elegantly': 2716,\n",
       " 'moaning': 8661,\n",
       " 'expectation': 2922,\n",
       " 'shakespearean': 9434,\n",
       " ...}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"vocab\\\\movie_vocab.pkl\",\"rb\")\n",
    "data = pickle.load(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.验证\n",
    "\n",
    "#### 8.1创建函数寻找验证单词周围的单词\n",
    "计算验证单词和所有词向量之间的余弦相似度\n",
    "$$cos(\\theta) = \\frac{a\\cdot b}{||a||\\times ||b||}= \\frac{\\sum_i^N(x_i\\times y_i)}{\\sqrt{\\sum_i^N(x_i)^2}\\times\\sqrt{\\sum_i^N(y_i)^2}}$$\n",
    "\n",
    "两个向量的余弦相似度计算：\n",
    "- 句子A：(1,2,1,1,1)\n",
    "- 句子B：(1,1,0,1,1)\n",
    "\n",
    "计算过程如下：\n",
    "\n",
    "$$cos(\\theta) = \\frac{1\\times1+2\\times1+1\\times0+1\\times1+1\\times1}{\\sqrt{1^2+2^2+1^2+1^2+1^2}\\times\\sqrt{1^2+1^2+0^2+1^2+1^2}}$$\n",
    "\n",
    "这里就是将分母先放在外面norm时完成，后面用matmul对两个向量中的每个数字对应相乘就好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keep_dims = True))\n",
    "normalized_embeddings = embeddings/norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings,normalized_embeddings,transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "预测词为： cliche\n",
      "预测的周围词为: kudos\n",
      "预测的周围词为: downer\n",
      "预测的周围词为: pool\n",
      "预测的周围词为: heartrending\n",
      "预测的周围词为: fame\n",
      "Nearest to cliche: kudos , downer , pool , heartrending , fame \n",
      "------------------------------\n",
      "预测词为： love\n",
      "预测的周围词为: payne\n",
      "预测的周围词为: mcdormand\n",
      "预测的周围词为: loquacious\n",
      "预测的周围词为: ranges\n",
      "预测的周围词为: devoid\n",
      "Nearest to love: payne , mcdormand , loquacious , ranges , devoid \n",
      "------------------------------\n",
      "预测词为： hate\n",
      "预测的周围词为: end\n",
      "预测的周围词为: substantial\n",
      "预测的周围词为: miscast\n",
      "预测的周围词为: tediously\n",
      "预测的周围词为: eve\n",
      "Nearest to hate: end , substantial , miscast , tediously , eve \n"
     ]
    }
   ],
   "source": [
    "batch_inputs, batch_labels = generate_batch_data(text_data, batch_size, windows_size)\n",
    "feed_dict = {x_inputs: batch_inputs, y_target: batch_labels}\n",
    "\n",
    "# Run the train step\n",
    "sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(),word_dictionary.keys()))\n",
    "# word_dictionary_rev = {0:'rare',1:'film',2:'movie'...}\n",
    "\n",
    "for j in range(len(valid_words)):\n",
    "    \n",
    "    valid_word = word_dictionary_rev[valid_example[j]]\n",
    "    print(\"-\"*30)\n",
    "    print(\"预测词为：\",valid_word)\n",
    "    log_str = \"Nearest to {}:\".format(valid_word)\n",
    "    topk = 5\n",
    "    nearst = (-sim[j,:]).argsort()[1:topk+1]\n",
    "    for k in range(topk):\n",
    "        print(\"预测的周围词为:\",word_dictionary_rev[nearst[k]])\n",
    "        \n",
    "        # 自己和自己迭代，就可以一直在后面添加了\n",
    "        log_str = \"%s %s ,\"%(log_str,word_dictionary_rev[nearst[k]])\n",
    "        \n",
    "    print(log_str[:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35-tf]",
   "language": "python",
   "name": "conda-env-py35-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
