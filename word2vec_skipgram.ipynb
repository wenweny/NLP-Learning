{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram\n",
    "\n",
    "- 数据集：电影评论数据集，下载地址：http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python.framework import ops\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "embedding_size = 200\n",
    "batch_size = 50\n",
    "windows_size = 2\n",
    "num_sampled = int(batch_size/2) #控制多少个批量转换为随机噪声\n",
    "epochs = 1000\n",
    "\n",
    "valid_words = ['cliche','love','hate']\n",
    "\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.读取导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['锘縯he rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \\n',\n",
       "  'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth . \\n',\n",
       "  'effective but too-tepid biopic\\n'],\n",
       " ['锘縮implistic , silly and tedious . \\n',\n",
       "  \"it's so laddish and juvenile , only teenage boys could possibly find it funny . \\n\",\n",
       "  'exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \\n'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file_path = 'dataset\\\\rt-polaritydata'\n",
    "pos_file = os.path.join(data_file_path,'rt-polarity.pos')# 正例\n",
    "neg_file = os.path.join(data_file_path,'rt-polarity.neg')# 负例\n",
    "\n",
    "#read data\n",
    "pos_data = []\n",
    "with open(pos_file,'r',encoding='gb18030',errors='ignore') as temp_pos_file:\n",
    "    for row in temp_pos_file:\n",
    "        pos_data.append(row)\n",
    "\n",
    "neg_data = []\n",
    "with open(neg_file,'r',encoding='gb18030',errors='ignore') as temp_neg_file:\n",
    "    for row in temp_neg_file:\n",
    "        neg_data.append(row)\n",
    "pos_data[:3],neg_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10661"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = pos_data+neg_data\n",
    "target = [1]*len(pos_data)+[0]*len(neg_data)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.数据预处理\n",
    "#### 2.1 转小写、去除标点数字和空白、去除“停词”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['锘縯he rock destined st centurys new conan hes going make splash even greater arnold schwarzenegger jeanclaud van damme steven segal',\n",
       " 'gorgeously elaborate continuation lord rings trilogy huge column words cannot adequately describe cowriterdirector peter jacksons expanded vision j r r tolkiens middleearth',\n",
       " 'effective tootepid biopic',\n",
       " 'sometimes like go movies fun wasabi good place start',\n",
       " 'emerges something rare issue movie thats honest keenly observed doesnt feel like one']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_text(texts):\n",
    "    stops = stopwords.words('english')#需要提前下载nltk_data，放在指定位置\n",
    "\n",
    "    # Lower case\n",
    "    texts = [x.lower() for x in texts]\n",
    "\n",
    "    # Remove punctuation\n",
    "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "\n",
    "    # Remove numbers\n",
    "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "\n",
    "    # Remove stopwords\n",
    "    texts = [' '.join([word for word in x.split() if word not in (stops)]) for x in texts]\n",
    "\n",
    "    # Trim extra whitespace\n",
    "    texts = [' '.join(x.split()) for x in texts]\n",
    "    \n",
    "    return(texts)\n",
    "\n",
    "texts = normalize_text(texts)\n",
    "texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 筛选影评长度大于3的数据，为更好确保影评的有效性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > 2]\n",
    "texts = [x for x in texts if len(x.split()) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.构建词汇表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计词频，取频次前voacabulary_size的词语和频次返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RARE', -1), ('film', 1445), ('movie', 1263), ('one', 726), ('like', 721)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = texts\n",
    "# vocabulary_size = 10000\n",
    "\n",
    "# Turn sentences (list of strings) into lists of words\n",
    "split_sentences = [s.split() for s in sentences]\n",
    "words = [x for sublist in split_sentences for x in sublist]\n",
    "\n",
    "# Initialize list of [word, word_count] for each word, starting with unknown\n",
    "count = [('RARE', -1)]\n",
    "\n",
    "# Now add most frequent words, limited to the N-most frequent (N=vocabulary size) \n",
    "# most_common 取出现频次最多的N个\n",
    "count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "count[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建词表，对每个词语赋予一个数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'story': 5, 'even': 7, 'comedy': 9, 'like': 4, 'movie': 2, 'one': 3, 'film': 1, 'much': 6, 'good': 8, 'RARE': 0}\n"
     ]
    }
   ],
   "source": [
    "# Now create the dictionary\n",
    "word_dictionary = {}\n",
    "# For each word, that we want in the dictionary, add it, then make it\n",
    "# the value of the prior dictionary length\n",
    "for i,(word, word_count) in enumerate(count):\n",
    "    word_dictionary[word] = len(word_dictionary)\n",
    "    #只是为了查看形成的word_dict是什么样的\n",
    "    if i==9:\n",
    "        print(word_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text to number\n",
    "对数据集中的单词映射数值，频次在前N个即在word_dict中，即映射相应数值，否则则映射rare的0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10405"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text to number\n",
    "text_data = []\n",
    "for sentence in texts:\n",
    "    sentence_data = []\n",
    "    for word in sentence.split():\n",
    "        if word in word_dictionary:\n",
    "            sentence_data.append(word_dictionary[word])\n",
    "        else:\n",
    "            sentence_data.append(0)\n",
    "    text_data.append(sentence_data)\n",
    "len(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择验证单词，创建验证单词的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1484, 28, 968]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在前N个词汇中选择\n",
    "valid_example = [word_dictionary[x] for x in valid_words]\n",
    "valid_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.生成skip-gram模型的批量数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1生成输入词对word_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3108, 2514, 111, 9, 39, 146, 165, 111, 807, 15],\n",
       " [[3108, 2514, 111],\n",
       "  [3108, 2514, 111, 9],\n",
       "  [3108, 2514, 111, 9, 39],\n",
       "  [2514, 111, 9, 39, 146],\n",
       "  [111, 9, 39, 146, 165],\n",
       "  [9, 39, 146, 165, 111],\n",
       "  [39, 146, 165, 111, 807],\n",
       "  [146, 165, 111, 807, 15],\n",
       "  [165, 111, 807, 15],\n",
       "  [111, 807, 15]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成窗口序列\n",
    "# windows_size = 2\n",
    "rand_sentence = np.random.choice(text_data)\n",
    "# rand_sentence_spilt = rand_sentence.split()\n",
    "rand_sentence_spilt = rand_sentence\n",
    "window_sequences=[rand_sentence_spilt[max((ix-windows_size),0):min((ix+windows_size+1),len(rand_sentence))] \n",
    "                  for ix,x in enumerate(rand_sentence_spilt)]\n",
    "rand_sentence,window_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(3108, [2514, 111]), (2514, [3108, 111, 9])],\n",
       " [(3108, 2514), (3108, 111), (2514, 3108), (2514, 111), (2514, 9)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找到中心词，根据中心词生成输入词对\n",
    "label_indices = [ix if ix<windows_size else windows_size for ix,x in enumerate(window_sequences)]\n",
    "batch_and_labels = [(x[y],x[:y]+x[(y+1):]) for x,y in zip(window_sequences,label_indices)]\n",
    "tuple_data = [(x,y_) for x,y in batch_and_labels for y_ in y]\n",
    "batch_and_labels[:2],tuple_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 生成批量数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data,label_data = [],[]\n",
    "# batch_size = 4\n",
    "word_input,labels_output = [list(x) for x in zip(*tuple_data)]\n",
    "batch_data.extend(word_input[:batch_size])\n",
    "label_data.extend(labels_output[:batch_size])\n",
    "\n",
    "batch_data = np.array(batch_data)\n",
    "label_data = np.transpose(np.array([label_data]))\n",
    "label_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 封装生成batch数据的函数\n",
    "为了在训练的时候调用，我们将上述分开的阶段封装成一个生成batch的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data(sentences,batch_size,windows_size):\n",
    "    batch_data,label_data = [],[]\n",
    "    \n",
    "    # 这个判断并不是没有意义的\n",
    "    # 可能句子本身的长度小于batch_size，所以此时如果直接截断是不够batch_size的\n",
    "    # 要重复extend，直到>batch_size\n",
    "    while len(batch_data) < batch_size:\n",
    "    \n",
    "        # 生成窗口序列,这里的sentences其实已经是转化成数字形式的了\n",
    "        rand_sentence = np.random.choice(sentences)\n",
    "\n",
    "        window_sequences=[rand_sentence[max((ix-windows_size),0):min((ix+windows_size+1),len(rand_sentence))] \n",
    "                          for ix,x in enumerate(rand_sentence)]\n",
    "\n",
    "        # 找到中心词，根据中心词生成输入词对\n",
    "        label_indices = [ix if ix<windows_size else windows_size for ix,x in enumerate(window_sequences)]\n",
    "        batch_and_labels = [(x[y],x[:y]+x[(y+1):]) for x,y in zip(window_sequences,label_indices)]\n",
    "        tuple_data = [(x,y_) for x,y in batch_and_labels for y_ in y]\n",
    "\n",
    "        # 生成批量数据\n",
    "        word_input,labels_output = [list(x) for x in zip(*tuple_data)]\n",
    "        \n",
    "        batch_data.extend(word_input[:batch_size])\n",
    "        label_data.extend(labels_output[:batch_size])\n",
    "        \n",
    "    # 如果句子长度小于batch大小，那需要几次进入while循环，extend直至大于，然后取前batch_size大小\n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "    \n",
    "    #转成数组格式以送入模型训练\n",
    "    bacth_data = np.array(batch_data)\n",
    "    #转置为了生成（batch,1)的大小\n",
    "    label_data = np.transpose(np.array([label_data]))\n",
    "    \n",
    "    return batch_data,label_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.初始化\n",
    "初始化嵌套函数，声明占位符和嵌套查找函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_3:0' shape=(10000, 200) dtype=float32_ref>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n",
    "\n",
    "x_inputs = tf.placeholder(tf.int32,shape=[batch_size])\n",
    "y_target = tf.placeholder(tf.int32,shape=[batch_size,1])\n",
    "valid_dataset = tf.constant(valid_example,dtype=tf.int32)\n",
    "\n",
    "embed = tf.nn.embedding_lookup(embeddings,x_inputs)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.定义NCE损失函数\n",
    "在skip-gram模型中，是对输入词求输出词语为想要词语的概率，实际是个分类问题。embedding_size是10000的话，意味着1万的高稀疏性分类。\n",
    "因此在这里使用噪声对比损失函数（noise-contrastive error,NCE)\n",
    "\n",
    "https://www.cnblogs.com/linhao-0204/p/9126037.html\n",
    "https://blog.csdn.net/wizardforcel/article/details/84075703"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size,embedding_size],stddev = 1.0/math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# 书里这个部分y_target和embed的位置写反了\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights,nce_biases,y_target,embed,num_sampled,vocabulary_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.构建模型\n",
    "#### 7.1声明优化器函数，初始化模型变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch100:77.96843719482422\n",
      "loss at epoch200:69.6238784790039\n",
      "loss at epoch300:53.15748977661133\n",
      "loss at epoch400:61.729766845703125\n",
      "loss at epoch500:25.412565231323242\n",
      "loss at epoch600:43.033470153808594\n",
      "loss at epoch700:42.40412139892578\n",
      "loss at epoch800:42.47169494628906\n",
      "loss at epoch900:49.41889953613281\n",
      "loss at epoch1000:20.44341278076172\n"
     ]
    }
   ],
   "source": [
    "loss_vec,loss_x_vec = [],[]\n",
    "for i in range(epochs):   \n",
    "    \n",
    "    batch_input,batch_labels = generate_batch_data(text_data,batch_size,windows_size)\n",
    "\n",
    "    feed_dict = {x_inputs:batch_input,y_target:batch_labels}\n",
    "    sess.run(optimizer,feed_dict)\n",
    "    \n",
    "    if (i+1)%100 ==0:\n",
    "        loss_val = sess.run(loss,feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print(\"loss at epoch{}:{}\".format(i+1,loss_val))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.验证\n",
    "\n",
    "#### 8.1创建函数寻找验证单词周围的单词\n",
    "计算验证单词和所有词向量之间的余弦相似度\n",
    "$$cos(\\theta) = \\frac{a\\cdot b}{||a||\\times ||b||}= \\frac{\\sum_i^N(x_i\\times y_i)}{\\sqrt{\\sum_i^N(x_i)^2}\\times\\sqrt{\\sum_i^N(y_i)^2}}$$\n",
    "\n",
    "两个向量的余弦相似度计算：\n",
    "- 句子A：(1,2,1,1,1)\n",
    "- 句子B：(1,1,0,1,1)\n",
    "\n",
    "计算过程如下：\n",
    "\n",
    "$$cos(\\theta) = \\frac{1\\times1+2\\times1+1\\times0+1\\times1+1\\times1}{\\sqrt{1^2+2^2+1^2+1^2+1^2}\\times\\sqrt{1^2+1^2+0^2+1^2+1^2}}$$\n",
    "\n",
    "这里就是将分母先放在外面norm时完成，后面用matmul对两个向量中的每个数字对应相乘就好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keep_dims = True))\n",
    "normalized_embeddings = embeddings/norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings,normalized_embeddings,transpose_b=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "送入模型计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30178624,  0.23389468,  0.28382635, ..., -0.03705493,\n",
       "       -0.025969  ,  0.04007516], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs, batch_labels = generate_batch_data(text_data, batch_size, windows_size)\n",
    "feed_dict = {x_inputs: batch_inputs, y_target: batch_labels}\n",
    "\n",
    "    # Run the train step\n",
    "sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "sim[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "预测词为： cliche\n",
      "预测的周围词为: along\n",
      "预测的周围词为: indeed\n",
      "预测的周围词为: scooter\n",
      "预测的周围词为: fanciful\n",
      "预测的周围词为: versus\n",
      "Nearest to cliche: along , indeed , scooter , fanciful , versus \n",
      "------------------------------\n",
      "预测词为： love\n",
      "预测的周围词为: la\n",
      "预测的周围词为: life\n",
      "预测的周围词为: stories\n",
      "预测的周围词为: american\n",
      "预测的周围词为: hours\n",
      "Nearest to love: la , life , stories , american , hours \n",
      "------------------------------\n",
      "预测词为： hate\n",
      "预测的周围词为: involving\n",
      "预测的周围词为: fairly\n",
      "预测的周围词为: embarrassment\n",
      "预测的周围词为: worthy\n",
      "预测的周围词为: boobs\n",
      "Nearest to hate: involving , fairly , embarrassment , worthy , boobs \n"
     ]
    }
   ],
   "source": [
    "word_dictionary_rev = dict(zip(word_dictionary.values(),word_dictionary.keys()))\n",
    "# word_dictionary_rev = {0:'rare',1:'film',2:'movie'...}\n",
    "for j in range(len(valid_words)):\n",
    "    \n",
    "    valid_word = word_dictionary_rev[valid_example[j]]\n",
    "    print(\"-\"*30)\n",
    "    print(\"预测词为：\",valid_word)\n",
    "    log_str = \"Nearest to {}:\".format(valid_word)\n",
    "    topk = 5\n",
    "    nearst = (-sim[j,:]).argsort()[1:topk+1]\n",
    "    for k in range(topk):\n",
    "        print(\"预测的周围词为:\",word_dictionary_rev[nearst[k]])\n",
    "        \n",
    "        # 自己和自己迭代，就可以一直在后面添加了\n",
    "        log_str = \"%s %s ,\"%(log_str,word_dictionary_rev[nearst[k]])\n",
    "    print(log_str[:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35-tf]",
   "language": "python",
   "name": "conda-env-py35-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
